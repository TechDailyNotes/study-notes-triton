{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOdQp8xQkCT/uRAsq/bqgCY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TechDailyNotes/study-notes-triton/blob/main/triton_02_fused_softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fused Softmax"
      ],
      "metadata": {
        "id": "zSwLl_-_dOiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivations"
      ],
      "metadata": {
        "id": "mjBUAIKVdVrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "-9XoR3nYidWT",
        "outputId": "0e0d8d9d-581f-41e4-d346-2814a3a9f061"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/\n",
            "Collecting triton-nightly\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/07c94329-d4c3-4ad4-9e6b-f904a60032ec/pypi/download/triton-nightly/3.post20240626041721/triton_nightly-3.0.0.post20240626041721-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (139.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton-nightly) (3.15.4)\n",
            "Installing collected packages: triton-nightly\n",
            "Successfully installed triton-nightly-3.0.0.post20240626041721\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "triton"
                ]
              },
              "id": "abe8c54050f54f50b9beb7c497d4840c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from triton.runtime import driver\n",
        "\n",
        "def naive_softmax(x):\n",
        "    # Step 1: Shift to prevent overflow.\n",
        "    # MN reads, M writes\n",
        "    x_max = x.max(dim=1)[0]\n",
        "    # MN + M reads, MN writes\n",
        "    z = x - x_max[:, None]\n",
        "\n",
        "    # Step 2: Compute softmax.\n",
        "    # MN reads, MN writes\n",
        "    numerator = z.exp()\n",
        "    # MN reads, M writes\n",
        "    denominator = numerator.sum(dim=1)\n",
        "    # MN + M reads, MN writes\n",
        "    retval = numerator / denominator[:, None]\n",
        "    # 5MN + 2M reads, 3MN + 2M writes\n",
        "    return retval"
      ],
      "metadata": {
        "id": "Ow5NlPujdW4u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Kernel"
      ],
      "metadata": {
        "id": "tbSiEOLjdXPQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "xcy279a_dKD0"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def softmax_kernel(input_ptr, output_ptr, n_rows, n_cols,\n",
        "                   input_row_stride, output_row_stride,\n",
        "                   BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr):\n",
        "    # Step 1: Get kernel parameters.\n",
        "    row_index_start = tl.program_id(0)\n",
        "    row_index_end = n_rows\n",
        "    row_index_step = tl.num_programs(0)\n",
        "\n",
        "    for row_index in tl.range(row_index_start, row_index_end, row_index_step,\n",
        "                              num_stages=num_stages):\n",
        "        # Step 2: Load data.\n",
        "        input_start_ptr = input_ptr + row_index * input_row_stride\n",
        "        input_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "        input_ptrs = input_start_ptr + input_offsets\n",
        "        input_mask = input_offsets < n_cols\n",
        "        input_row = tl.load(input_ptrs, mask=input_mask, other=-float(\"inf\"))\n",
        "\n",
        "        # Step 3: Compute data.\n",
        "        input_row_ = input_row - tl.max(input_row, axis=0)\n",
        "        numerator = tl.exp(input_row_)\n",
        "        denominator = tl.sum(numerator, axis=0)\n",
        "        output_row = numerator / denominator\n",
        "\n",
        "        # Step 4: Store data.\n",
        "        output_start_ptr = output_ptr + row_index * output_row_stride\n",
        "        output_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "        output_ptrs = output_start_ptr + output_offsets\n",
        "        output_mask = output_offsets < n_cols\n",
        "        tl.store(output_ptrs, output_row, mask=output_mask)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.cuda.current_device()\n",
        "properties = driver.active.utils.get_device_properties(device)\n",
        "MAX_SHARED_MEM = properties[\"max_shared_mem\"]\n",
        "WARP_SIZE = properties[\"warpSize\"]\n",
        "MAX_NUM_REGS = properties[\"max_num_regs\"]\n",
        "SM_COUNT = properties[\"multiprocessor_count\"]\n",
        "target = triton.runtime.driver.active.get_current_target()\n",
        "\n",
        "def softmax(x):\n",
        "    # Step 1: Init output.\n",
        "    y = torch.empty_like(x)\n",
        "\n",
        "    # Step 2: Set kernel parameters.\n",
        "    n_rows, n_cols = x.shape\n",
        "\n",
        "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
        "    num_stages = 4 if MAX_SHARED_MEM > 200000 else 2\n",
        "    num_warps = 8\n",
        "\n",
        "    kernels = {}\n",
        "\n",
        "    # Step 3: Launch kernel function.\n",
        "    kernel, n_programs = kernels.get(BLOCK_SIZE, (None, 0))\n",
        "\n",
        "    if kernel is None:\n",
        "        kernel = softmax_kernel.warmup(x, y, n_rows, n_cols,\n",
        "                                       x.stride(0), y.stride(0),\n",
        "                                       BLOCK_SIZE=BLOCK_SIZE,\n",
        "                                       num_stages=num_stages,\n",
        "                                       num_warps=num_warps, grid=(1, ))\n",
        "        kernel._init_handles()\n",
        "\n",
        "        occupancy = min(MAX_NUM_REGS // (num_warps * WARP_SIZE * kernel.n_regs),\n",
        "                        MAX_SHARED_MEM // kernel.metadata.shared)\n",
        "        n_programs = SM_COUNT * occupancy\n",
        "\n",
        "        kernels[BLOCK_SIZE] = (kernel, n_programs)\n",
        "\n",
        "    n_programs = min(n_programs, n_rows)\n",
        "\n",
        "    kernel[(n_programs, 1, 1)](x, y, n_rows, n_cols, x.stride(0), y.stride(0))\n",
        "\n",
        "    # Step 4: Return output.\n",
        "    return y\n",
        "\n",
        "# device = torch.cuda.current_device()\n",
        "# properties = driver.active.utils.get_device_properties(device)\n",
        "# NUM_SM = properties[\"multiprocessor_count\"]\n",
        "# NUM_REGS = properties[\"max_num_regs\"]\n",
        "# SIZE_SMEM = properties[\"max_shared_mem\"]\n",
        "# WARP_SIZE = properties[\"warpSize\"]\n",
        "# target = triton.runtime.driver.active.get_current_target()\n",
        "# kernels = {}\n",
        "\n",
        "\n",
        "# def softmax(x):\n",
        "#     n_rows, n_cols = x.shape\n",
        "\n",
        "#     # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
        "#     BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
        "\n",
        "#     # Another trick we can use is to ask the compiler to use more threads per row by\n",
        "#     # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
        "#     # You will see in the next tutorial how to auto-tune this value in a more natural\n",
        "#     # way so you don't have to come up with manual heuristics yourself.\n",
        "#     num_warps = 8\n",
        "\n",
        "#     # Number of software piepling stages.\n",
        "#     num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
        "\n",
        "#     # Allocate output\n",
        "#     y = torch.empty_like(x)\n",
        "\n",
        "#     # pre-compile kernel to get register usage and compute thread occupancy.\n",
        "#     kernel, num_programs = kernels.get(BLOCK_SIZE, (None, 0))\n",
        "#     if kernel is None:\n",
        "#         kernel = softmax_kernel.warmup(x, y, n_rows, n_cols, x.stride(0), y.stride(0), BLOCK_SIZE=BLOCK_SIZE,\n",
        "#                                        num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
        "#         kernel._init_handles()\n",
        "#         n_regs = kernel.n_regs\n",
        "#         size_smem = kernel.metadata.shared\n",
        "#         occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
        "#         occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
        "#         num_programs = NUM_SM * occupancy\n",
        "#         kernels[BLOCK_SIZE] = (kernel, num_programs)\n",
        "\n",
        "#     num_programs = min(num_programs, n_rows)\n",
        "\n",
        "#     # Create a number of persistent programs.\n",
        "#     kernel[(num_programs, 1, 1)](\n",
        "#         x,\n",
        "#         y,\n",
        "#         n_rows,\n",
        "#         n_cols,\n",
        "#         x.stride(0),\n",
        "#         y.stride(0),\n",
        "#     )\n",
        "#     return y"
      ],
      "metadata": {
        "id": "1YB9QA4QjML1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit Test"
      ],
      "metadata": {
        "id": "geNeN94wdYl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "x = torch.randn((1823, 781), dtype=torch.float32, device=\"cuda\")\n",
        "output_triton = softmax(x)\n",
        "output_torch = torch.softmax(x, axis=1)\n",
        "assert torch.allclose(output_triton, output_torch), (output_triton, output_torch)\n",
        "print(f\"Max diff is {torch.max(torch.abs(output_triton - output_torch))}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KguzEsz4daY9",
        "outputId": "1b10324e-49e6-472f-c45c-30696bea7610"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max diff is 7.450580596923828e-09.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark"
      ],
      "metadata": {
        "id": "MrvGR7BMdamG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B7Qel-MMda5o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}